{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import GPT\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mengfanxu/miniconda3/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading weights from pretrained gpt: gpt2-xl\n",
      "forcing vocab_size=50257, block_size=1024, bias=True\n",
      "number of parameters: 1555.97M\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3186120/3626383437.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(\"orthogonal/gpt2-xl.pt\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GPT.from_pretrained('/data2/mengfanxu/huggingface/gpt2-xl')\n",
    "orthogonal_model = deepcopy(model)\n",
    "state_dict = torch.load(\"orthogonal/gpt2-xl.pt\")\n",
    "orthogonal_model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48, 1600)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orthogonal_model.config.n_layer,orthogonal_model.config.n_embd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "qk_norm_list = []\n",
    "vo_norm_list = []\n",
    "for name, module in orthogonal_model.named_modules():\n",
    "    if name.endswith(\"attn\"):\n",
    "        q_weight = deepcopy(module.q_proj.weight.data) # (hidden_size, q_in_dim)\n",
    "        q_bias = deepcopy(module.q_proj.bias.data).unsqueeze(1) # (hidden_size, 1)\n",
    "        q_weight = torch.cat([q_weight, q_bias],dim=1)  # (hidden_size, q_in_dim+1)\n",
    "        \n",
    "        k_weight = deepcopy(module.k_proj.weight.data) # (hidden_size, k_in_dim)\n",
    "        k_bias = deepcopy(module.k_proj.bias.data).unsqueeze(1) # (hidden_size, 1)\n",
    "        k_weight = torch.cat([k_weight, k_bias],dim=1)  # (hidden_size, k_in_dim+1)\n",
    "        qk_norm = q_weight.norm(p=2,dim=-1) * k_weight.norm(p=2,dim=-1)\n",
    "        qk_norm_list.append(qk_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "qk_norm_list = torch.stack(qk_norm_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([75.6407, 59.7586, 34.3032,  ...,  0.7776,  0.7452,  0.6627])\n",
      "tensor([24.5278, 14.9114, 11.4587,  ...,  0.6109,  0.5813,  0.5491])\n",
      "tensor([33.4253, 19.3119, 12.8235,  ...,  0.7925,  0.7374,  0.6017])\n",
      "tensor([12.3362, 10.8317,  9.7711,  ...,  1.1934,  0.8758,  0.8080])\n",
      "tensor([6.9461, 5.2508, 4.8550,  ..., 1.2732, 0.8975, 0.8220])\n",
      "tensor([8.8811, 7.5086, 6.9884,  ..., 1.4820, 0.9754, 0.7467])\n",
      "tensor([8.8682, 6.5936, 5.0357,  ..., 1.4805, 1.4434, 1.4173])\n",
      "tensor([11.4886,  8.5152,  8.1921,  ...,  2.0938,  2.0079,  1.8927])\n",
      "tensor([9.0519, 8.7206, 7.3931,  ..., 2.2672, 0.7412, 0.7057])\n",
      "tensor([11.4003, 10.5980,  7.0392,  ...,  1.6157,  1.5513,  1.4216])\n",
      "tensor([10.2905, 10.0548,  7.5150,  ...,  2.3830,  2.2690,  2.2384])\n",
      "tensor([8.2963, 6.5400, 5.9895,  ..., 2.3059, 2.1929, 2.0998])\n",
      "tensor([7.9127, 7.0393, 6.8282,  ..., 2.7395, 2.6665, 2.6385])\n",
      "tensor([6.8908, 5.9467, 5.7999,  ..., 1.8580, 1.8460, 1.5443])\n",
      "tensor([6.9871, 5.6310, 5.0021,  ..., 1.9697, 1.8825, 1.8153])\n",
      "tensor([7.9711, 5.9890, 5.5717,  ..., 1.8205, 1.6905, 1.5868])\n",
      "tensor([6.1324, 5.3459, 4.8864,  ..., 1.9139, 1.7434, 1.7101])\n",
      "tensor([5.1001, 4.7953, 4.5182,  ..., 2.1310, 2.0617, 1.9097])\n",
      "tensor([10.4053,  6.8618,  6.4308,  ...,  1.8255,  1.7466,  1.4522])\n",
      "tensor([6.0008, 5.7598, 4.6868,  ..., 1.8456, 1.8151, 1.6755])\n",
      "tensor([8.5221, 5.9770, 5.8315,  ..., 1.9341, 1.8986, 1.8642])\n",
      "tensor([5.7272, 4.9186, 4.5510,  ..., 0.5434, 0.5053, 0.4947])\n",
      "tensor([5.7100, 5.1505, 4.7474,  ..., 1.5297, 1.3650, 1.1859])\n",
      "tensor([5.5530, 5.4290, 4.7280,  ..., 2.0089, 1.9062, 1.7743])\n",
      "tensor([6.6216, 4.3648, 4.2366,  ..., 2.1615, 2.1159, 2.0120])\n",
      "tensor([6.2801, 5.4814, 4.7062,  ..., 1.7604, 1.6685, 1.6296])\n",
      "tensor([6.6259, 5.3699, 5.0051,  ..., 1.8606, 1.7071, 1.6458])\n",
      "tensor([6.2396, 4.9255, 4.2530,  ..., 1.7886, 1.7107, 1.6269])\n",
      "tensor([5.1002, 4.6615, 4.4633,  ..., 1.9765, 1.9037, 1.7801])\n",
      "tensor([5.3510, 4.9490, 4.4481,  ..., 1.7975, 1.7468, 1.5995])\n",
      "tensor([6.3054, 4.8892, 4.5223,  ..., 1.9279, 1.8954, 1.6093])\n",
      "tensor([6.0375, 5.3276, 5.2673,  ..., 1.8403, 1.7218, 1.6496])\n",
      "tensor([6.2456, 4.6941, 4.1177,  ..., 1.7818, 1.7025, 1.6006])\n",
      "tensor([6.3919, 6.0235, 4.3141,  ..., 2.1301, 1.9534, 1.8290])\n",
      "tensor([5.4671, 4.3424, 4.1377,  ..., 1.7720, 1.7365, 1.6971])\n",
      "tensor([4.6442, 3.9674, 3.4904,  ..., 1.8272, 1.7875, 1.6677])\n",
      "tensor([4.8054, 4.3386, 3.8780,  ..., 2.0930, 2.0544, 1.9553])\n",
      "tensor([4.6713, 4.1048, 3.8005,  ..., 1.8251, 1.7724, 1.6564])\n",
      "tensor([5.4229, 3.6836, 3.4500,  ..., 2.0196, 1.9065, 1.8043])\n",
      "tensor([5.7805, 4.7131, 4.1463,  ..., 2.0297, 1.9733, 1.9401])\n",
      "tensor([5.1915, 4.0667, 3.6411,  ..., 1.8216, 1.7510, 1.6517])\n",
      "tensor([4.8824, 3.9771, 3.8413,  ..., 1.8360, 1.8299, 1.6578])\n",
      "tensor([5.5084, 4.6543, 3.9121,  ..., 1.5290, 1.4225, 1.3273])\n",
      "tensor([4.6926, 4.3665, 4.1449,  ..., 1.6826, 1.6089, 1.3480])\n",
      "tensor([5.0422, 4.1576, 3.9641,  ..., 2.0209, 1.9845, 1.6809])\n",
      "tensor([5.2633, 4.8817, 4.4769,  ..., 1.9256, 1.8613, 1.8058])\n",
      "tensor([5.8213, 5.1518, 4.8696,  ..., 2.1709, 1.8128, 1.7171])\n",
      "tensor([12.9431,  6.1117,  4.9405,  ...,  2.2587,  2.1826,  2.0790])\n"
     ]
    }
   ],
   "source": [
    "for norm in qk_norm_list:\n",
    "    print(norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "dataset = load_from_disk(\"wikitext-2-raw-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 4358\n",
       "})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Can't load tokenizer for '/data2/mengfanxu/huggingface/gpt2-xl'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/data2/mengfanxu/huggingface/gpt2-xl' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m model_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/data2/mengfanxu/huggingface/gpt2-xl\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m model \u001b[38;5;241m=\u001b[39m GPT2LMHeadModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_id)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 7\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mGPT2TokenizerFast\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_from_disk\n\u001b[1;32m     11\u001b[0m test \u001b[38;5;241m=\u001b[39m load_from_disk(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwikitext-2-raw-v1\u001b[39m\u001b[38;5;124m\"\u001b[39m, split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2016\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2013\u001b[0m \u001b[38;5;66;03m# If one passes a GGUF file path to `gguf_file` there is no need for this check as the tokenizer will be\u001b[39;00m\n\u001b[1;32m   2014\u001b[0m \u001b[38;5;66;03m# loaded directly from the GGUF file.\u001b[39;00m\n\u001b[1;32m   2015\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(full_file_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m full_file_name \u001b[38;5;129;01min\u001b[39;00m resolved_vocab_files\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m gguf_file:\n\u001b[0;32m-> 2016\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m   2017\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt load tokenizer for \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. If you were trying to load it from \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2018\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, make sure you don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt have a local directory with the same name. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2019\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOtherwise, make sure \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is the correct path to a directory \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2020\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontaining all relevant files for a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m tokenizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2021\u001b[0m     )\n\u001b[1;32m   2023\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_id, file_path \u001b[38;5;129;01min\u001b[39;00m vocab_files\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   2024\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m file_id \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m resolved_vocab_files:\n",
      "\u001b[0;31mOSError\u001b[0m: Can't load tokenizer for '/data2/mengfanxu/huggingface/gpt2-xl'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/data2/mengfanxu/huggingface/gpt2-xl' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer."
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2TokenizerFast\n",
    "from accelerate.test_utils.testing import get_backend\n",
    "\n",
    "device = 'cpu'\n",
    "model_id = \"/data2/mengfanxu/huggingface/gpt2-xl\"\n",
    "model = GPT2LMHeadModel.from_pretrained(model_id).to(device)\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(model_id)\n",
    "\n",
    "from datasets import load_from_disk\n",
    "test = load_from_disk(\"wikitext-2-raw-v1\", split=\"test\")\n",
    "encodings = tokenizer(\"\\n\\n\".join(test[\"text\"]), return_tensors=\"pt\")\n",
    "\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "max_length = model.config.n_positions\n",
    "stride = 512\n",
    "seq_len = encodings.input_ids.size(1)\n",
    "\n",
    "nll_sum = 0.0\n",
    "n_tokens = 0\n",
    "prev_end_loc = 0\n",
    "for begin_loc in tqdm(range(0, seq_len, stride)):\n",
    "    end_loc = min(begin_loc + max_length, seq_len)\n",
    "    trg_len = end_loc - prev_end_loc  # may be different from stride on last loop\n",
    "    input_ids = encodings.input_ids[:, begin_loc:end_loc].to(device)\n",
    "    target_ids = input_ids.clone()\n",
    "    target_ids[:, :-trg_len] = -100\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, labels=target_ids)\n",
    "\n",
    "        # loss is calculated using CrossEntropyLoss which averages over valid labels\n",
    "        # N.B. the model only calculates loss over trg_len - 1 labels, because it internally shifts the labels\n",
    "        # to the left by 1.\n",
    "        neg_log_likelihood = outputs.loss\n",
    "\n",
    "    # Accumulate the total negative log-likelihood and the total number of tokens\n",
    "    num_valid_tokens = (target_ids != -100).sum().item()  # number of valid tokens in target_ids\n",
    "    batch_size = target_ids.size(0)\n",
    "    num_loss_tokens = num_valid_tokens - batch_size  # subtract batch_size due to internal label shift\n",
    "    nll_sum += neg_log_likelihood * num_loss_tokens\n",
    "    n_tokens += num_loss_tokens\n",
    "\n",
    "    prev_end_loc = end_loc\n",
    "    if end_loc == seq_len:\n",
    "        break\n",
    "\n",
    "avg_nll = nll_sum / n_tokens  # average negative log-likelihood per token\n",
    "ppl = torch.exp(avg_nll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
